{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing \n",
    "\n",
    "An overview of parallel processing with CASA\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization concept \n",
    "\n",
    "Introduction to CASA\\'s parallelization concept\n",
    "\n",
    " The parallelization approach adopted in CASA is the so-called embarrassingly parallelization. Embarassingly parallel workload or problem is one where little or no effort is needed to separate the problem into a number of parallel tasks. This is often the case where there is little or no dependency or need for communication between those parallel tasks, or for results between them.\n",
    "\n",
    "In order to run one analysis on multiple processors, one can parallelize the work by dividing the data into several parts (\"partitioning\") and then run a CASA instance on each part or[ have non-trivially parallelized algorithms, which make use of several processors within a single CASA instance. Non-trivial parallelization is presently only implemented in certain sections of the imaging code of CASA based on [OpenMP](http://www.openmp.org/ \"OpenMP\"), which is a shared-memory parallel programming library.]{\n",
    "\n",
    "All other parallelization is achieved by partitioning the MeasurementSet (MS) of interest using the task **partition** or at import time using **importasdm**[. The resulting partitioned MS is called a \"Multi-MS\" or \"MMS\". The parallel processing of a Multi-MS is possible using the Message Passing Interface ([MPI](http://mpi-forum.org/ \"MPI\")). MPI is a standard which addresses primarily the message-passing parallel programming model in a practical, portable, efficient and flexible way.]{\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**WARNING***:* Parallel processing on multi-MSs in CASA is unverified - please use at own discretion. \n",
    "</div>\n",
    "\n",
    "Logically, an MMS has the same structure as an MS but internally it is a group of several MSs which are virtually concatenated. Virtual concatenation of several MSs or MMSs into an MMS can also be achieved via task **virtualconcat**.\n",
    "\n",
    "Due to the virtual concatenation, the main table of an MMS appears like the union of the main tables of all the member MSs such that when the MMS is accessed like a normal MS, processing can proceed sequentially as usual. Each member MS or \"Sub-MS\" of an MMS, however, is at the same time a valid MS on its own and can be processed as such. This is what happens when the MMS is accessed by a parallelized task. The partitioning of the MMS is recognized and work is started in parallel on the separate Sub-MSs, provided that the user has started CASA with mpicasa. \n",
    "\n",
    "The internal structure of an MMS can be inspected using task **listpartition**.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control & Configuration \n",
    "\n",
    "Starting CASA with MPI and requirements for running CASA in parallel. The mpi4casa framework.\n",
    "\n",
    "##### Requirements \n",
    "\n",
    " CASA can be run in parallel on a cluster of computer nodes or on a single multi-core computer. In the multi-node case, the following requirements are necessary for all nodes to be included in the cluster. Users with access to a cluster will not need to do these settings, but it is still useful to be aware of the configuration:\n",
    "\n",
    "-   Password-less ssh access from the client (user) machine into all the nodes to be included in the cluster.\n",
    "    <div class=\"alert alert-info\">\n",
    "    NOTE: This is not necessary when using only localhost, i.e. if the cluster is deployed only on the machine where CASA is running.\n",
    "    </div>\n",
    "-   All the input files must be located in a shared file-system, accessible from all the nodes comprising the cluster, and mounted in the same path of the file-system.\n",
    "-   Mirrored CASA installation with regards to the CASA installation in the client (user) machine, so that the following environmental variables are pointing to valid installations: PATH, LD_LIBRARY_PATH, IPYTHONDIR, CASAPATH, CASAARCH, PYTHONHOME, \\_\\_CASAPY_PYTHONDIR, PGPLOT_DEV, PGPLOT_DIR, PGPLOT_FONT. This is usually achieved by having the CASA installation on a shared file-system.    \n",
    "\n",
    " \n",
    "\n",
    "##### Configuration and Start-Up \n",
    "\n",
    " [The main library used in CASA (4.4+) to achieve parallelization is the Message Passing Interface (MPI) and in particular the [OpenMPI](http://www.open-mpi.org \"OpenMPI\") implementation. MPI is already included in the CASA distribution so that users do not need to install it. The CASA distribution comes with a wrapper of the MPI executor, which is called *mpicasa*]{. This wrapper does several settings behind the scenes in order to properly configure the environment to run CASA in parallel.\n",
    "\n",
    "The collection of CASA processes which will run the jobs from parallelized tasks, is set up via *mpicasa*. The simplest example is to run CASA in parallel on the *localhost* using the available cores in the machine. A typical example would be to run CASA on a desktop with 16 cores such as the following example:\n",
    "\n",
    "```\n",
    "path_to_casa/mpicasa -n 16 path_to_casa/casa <casa_options>\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "1.  *mpicasa*: Wrapper around *mpirun*, which can be found in the casa installation directory. Example: /home/user/casa-release-4.5.0-el6/bin\n",
    "2.  *-n* : MPI option to get the number of processes to run.\n",
    "3.  *16*: The number of cores to be used in the localhost machine.\n",
    "    <div class=\"alert alert-info\">\n",
    "    NOTE: MPI uses one process as the MPI Client, which is where the user will see messages printed in the terminal or in the logger. The other processes are used for the parallel work and are called MPI Servers. Because of this, usually we give number_of_processes + 1.\n",
    "    </div>\n",
    "4.  *casa*: Full path to the CASA executable, *casa*.\n",
    "5.  *casa_options*: CASA options such as: *-c*, *--nogui*, *--log2term*, etc. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "NOTE: when several versions of CASA are available in the PATH, there is the risk that the executable mpicasa and other executables used by CASA, such as casaplotms or asdm2MS, would be picked from one of those different versions instead of the \"path_to_casa/casa\" version that we want to run. This is typically the case in data reduction clusters where either the default environment or user setup scripts set the PATH to point to the latest release of CASA, for example. In such cases, it is safer to make sure in advance that the first version found in the PATH is the right one, with a command like this (bash), as explained in the CASA distribution README: \n",
    "\n",
    "*export PATH=path_to_casa/bin:$PATH*\n",
    "</div>\n",
    "\n",
    "It is also possible to use other nodes, which can form a \"cluster\". Following the requirements given above, replace the \"*-n*\" option of *mpicasa* with a \"*-hostfile host_file*\", as shown below:\n",
    "\n",
    "```\n",
    "mpicasa -hostfile <host_file> path_to_casa/casa <casa_options>\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "1.  *host_file*: It is a text file containing the name of the nodes forming the cluster and the number of cores to use in each one of the nodes. \n",
    "\n",
    "Example:\n",
    "\n",
    "``` {.verbatim}\n",
    "orion slots=5\n",
    "antares slots=4\n",
    "sirius slots=4\n",
    "```\n",
    "\n",
    "The above configuration file will set up a cluster comprised of three nodes (orion, antares and sirius), deploying the cores per node as follows: At host \"orion\" up to 5 cores will be deployed (including the MPI Client). If the processing requires more cores, it will take them from \"antares\" and once all the 4 engines in \"antares\" are used, it will use up to 4 cores in \"sirius\".\n",
    "\n",
    " \n",
    "\n",
    "To run CASA in interactive mode (without the \\\"-c\\\" option) the user needs to first login to the desired computer node with X11 forwarding. This is achieved with the command    *ssh -XY \\<node\\>,* where \\<node\\> is the hostname of the computer where he/she wants to run CASA.  * *\n",
    "\n",
    "```\n",
    "mpicasa -n <number_of_processes> path_to_casa/casa\n",
    "```\n",
    "\n",
    "This will open an *xterm* window for the interactive work. To get help do:\n",
    "\n",
    "```\n",
    "mpicasa --help\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Imaging \n",
    "\n",
    "Parallel imaging in tclean\n",
    "\n",
    "The parallelization of imaging is achieved through task **tclean**. The parallelization itself is tied closely to the major-minor cycles of the imager and follows a different approach of that used by other tasks. The parallelization inside **tclean** does not need the MS to be partitionted into a Multi-MS. It will work in the same way if the input is an MS or MMS. But in order to run **tclean** in parallel it is necessary to launch CASA with **mpicasa**, in the same way as for other tasks. One extra step necessary to run **tclean** in parallel is to set the parameter *parallel=True*. All the details of the parallelization are described in the section mentioned above.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Parallel imaging on an MS file (rather than MMS file) in tclean is an official mode of operations in the ALMA pipeline since Cycle-6, and officially endorsed by CASA as per CASA 5.4. We recommend users interested in parallel processing to use this mode of operation. For large data products, the imaging step often dominates the overall runtime, as well as the advantages that can be achieved with parallelization (see [CASA Memo 5](https://casa.nrao.edu/casadocs-devel/stable/memo-series/casa-memos)). Processing Multi-MS files, either for imaging or calibration, remains at the discretion of the user.\n",
    "</div>\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multi-MS \n",
    "\n",
    "Describing a Multi-MS and how to create one\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parallel processing using Multi-MS (MMS) in CASA is unverified. Please use at own discretion. **\n",
    "\n",
    "##### [Please consider [parallel imaging](https://casa.nrao.edu/casadocs-devel/stable/parallel-processing/parallel-imaging) using normal MS as alternative.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-MS Creation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **partition** \n",
    "\n",
    "The **partition** task is the main task to create a \"Multi-MS\". It takes an input MeasurementSet and creates an output \"Multi-MS\" based on the data selection parameters.\n",
    "\n",
    "The inputs to **partition** are:\n",
    "\n",
    "```\n",
    "CASA <1>: inp partition\n",
    "--------> inp(partition)\n",
    "#partition :: Task to produce Multi-MSs using parallelism\n",
    "vis                 =         ''        #Name of input MeasurementSet\n",
    "outputvis           =         ''        #Name of output MeasurementSet\n",
    "createmms           =       True        #Should this create a multi-MS output\n",
    "     separationaxis =     'auto'        #Axis to do parallelization across(scan, spw, baseline, auto)\n",
    "     numsubms       =     'auto'        #The number of SubMSs to create (auto or any number)\n",
    "     flagbackup     =       True        #Create a backup of the FLAG column in the MMS.\n",
    "\n",
    "datacolumn          =      'all'        #Which data column(s) to process.\n",
    "field               =         ''        #Select field using ID(s) or name(s).\n",
    "spw                 =         ''        #Select spectral window/channels.\n",
    "scan                =         ''        #Select data by scan numbers.\n",
    "antenna             =         ''        #Select data based on antenna/baseline.\n",
    "correlation         =         ''        #Correlation: '' ==> all, correlation='XX,YY'.\n",
    "timerange           =         ''        #Select data by time range.\n",
    "intent              =         ''        #Select data by scan intent.\n",
    "array               =         ''        #Select (sub)array(s) by array ID number.\n",
    "uvrange             =         ''        #Select data by baseline length.\n",
    "observation         =         ''        #Select by observation ID(s).\n",
    "feed                =         ''        #Multi-feed numbers: Not yet implemented.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The *createmms* parameter \n",
    "\n",
    " The keyword *createmms* is by default set to True to create an output MMS. It contains three sub-parameters, *separationaxis*, *numsubms* and *flagbackup*. Partition accepts four axes to do separation across: 'auto', 'scan' 'spw' or 'baseline'. The default *separationaxis=\\'auto\\'* will first separate the MS in spws, then in scans. It tries to balance the spw and scan content in each Sub-MS also taking into account the available fields.\n",
    "\n",
    "The baseline axis is mostly useful for Single-Dish data. This axis will partition the MS based on the available baselines. If the user wants only auto-correlations, she/he should use the antenna selection syntax such as *antenna=\\'\\*&&&\\'* together with the baseline separation axis. Note that if *numsubms=\\'auto\\'*, the task will try to create as many Sub-MSs as the number of available parallel cores used when starting CASA with *mpicasa*. If the user wants to have one Sub-MS for each baseline, he/she should set the *numsubms* parameter to a number higher than the number of baselines to achieve this.\n",
    "\n",
    "The user may force the number of \"Sub-MSs\" in the output MMS by setting the sub-parameter numsubms. The default *\\'auto\\'* is to create as many Sub-MSs as the number of engines used when starting CASA with *mpicasa*, in an optimized way. \n",
    "\n",
    "The *flagbackup* sub-parameter will create a backup of the FLAG column and save it to the .flagversions file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **importasdm** \n",
    "\n",
    "Task **partition** has been embedded in task **importasdm** so that at import time the user can already create a MMS. Set the parameter createmms to True and the output of **importasdm** will be a MMS created with default parameters. Sub-parameters separationaxis and numsubms are also available in **importasdm**. From this point on in the data reduction chain, tasks that have been parallelized will run automatically in parallel when they see an MMS and tasks that are not parallelized will work in the same way as they normally do on a MS.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Calibration \n",
    "\n",
    "List of internally parallelized calibration tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel processing using Multi-MS (MMS) in CASA is unverified - please use at own discretion. \n",
    "\n",
    "##### [Please consider [[parallel imaging](https://casa.nrao.edu/casadocs-devel/stable/parallel-processing/parallel-imaging)]{ using normal MS as alternative.]{\n",
    "\n",
    " \n",
    "\n",
    "Some of the calibration tasks are internally parallelized and will run in parallel if the input MS is a Multi-MS. Other tasks are not and will work normally in the presence of an input MMS. A typical calibration cascade will work normally in parallel when it sees an input MMS. In order to do that, the first step is to set *createmms=True* inside **importasdm** to create a Multi-MS. Once that is done, the calibration steps will distribute the processing in parallel if CASA is started with **mpicasa**, or in serial otherwise.\n",
    "\n",
    " \n",
    "\n",
    "Contrary to the MS, the calibration tables created by calibration tasks are not partitioned. For instance, when **gaincal** is run on a Multi-MS, it will create the same output **gaincal** table as if the input was a normal MS.\n",
    "\n",
    " \n",
    "\n",
    "The following calibration tasks are internally parallelised and will work on each Sub-MS in parallel.\n",
    "\n",
    "flagdata\n",
    "\n",
    "setjy\n",
    "\n",
    "applycal\n",
    "\n",
    "hanningsmooth\n",
    "\n",
    "cvel2\n",
    "\n",
    "uvcontsub\n",
    "\n",
    "mstransform\n",
    "\n",
    "split\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special considerations when running some tasks in parallel {#special-considerations-when-running-some-tasks-in-parallel \n",
    "\n",
    "##### uvcontsub {#uvcontsub \n",
    "\n",
    "When the input is a [Multi-MS](https://casa.nrao.edu/casadocs-devel/stable/parallel-processing/the-multi-ms) and CASA is started in parallel using [mpicasa](https://casa.nrao.edu/casadocs-devel/stable/parallel-processing/parallelization-control), **uvcontsub** will try to process each Sub-MS in parallel. Depending on the parameters of uvcontsub and the separation axis of the partitioned Multi-MS, processing the input in parallel is not possible. This will happen for example when the input MMS is separated using the default axis \\'auto\\'. The \\'auto\\' axis will partition the MMS  by the scan and spw axes, in a way to balance the content on each Sub-MS.\n",
    "\n",
    " \n",
    "\n",
    "If **uvcontsub** is called with combine=\\'spw\\', the task will expect to find all selected spws in each Sub-MS, as each parallel engine will process a Sub-MS independently of the others. In such cases, task uvcontsub will issue some warnings that the process cannot be continued in parallel. The task will internally handle such cases and will continue to process the input in serial, as if the Multi-MS was a normal monolithic MS.\n",
    "\n",
    " \n",
    "\n",
    "The following steps can be informed in order to find out what is the partition axis of the MMS and what is the content of each Sub-MS. First, use task [listpartition](https://casa.nrao.edu/casadocs-devel/stable/global-task-list/task_listpartition) to obtain information on the MMS.\n",
    "\n",
    "```\n",
    "CASA <2>: listpartition('combspw.mms')\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "INFO listpartition\n",
    "```\n",
    "\n",
    "In the above example, the MMS was partitioned using the default axis \\'auto\\' (scan,spw). One can see the Sub-MSs do not contain all spws, therefore depending on the selection used in the task, it will not be possible to proceed in parallel. See the following example for the warnings given by the task in this case.\n",
    "\n",
    "```\n",
    "CASA <8>: uvcontsub(vis=\"combspw.mms\",fitspw=\"1~10:5~122,15~22:5~122\",excludechans=False,combine=\"spw\",fitorder=0,spw=\"6~14\",want_cont=False)\n",
    "2018-02-06 15:45:09 INFO uvcontsub\n",
    "2018-02-06 15:45:09 INFO uvcontsub\n",
    "2018-02-06 15:45:09 INFO uvcontsub\n",
    "2018-02-06 15:45:09 INFO uvcontsub\n",
    "2018-02-06 15:45:09 INFO uvcontsub\n",
    "[2018-02-06 15:45:11 WARN uvcontsub\n",
    "[2018-02-06 15:45:11 WARN uvcontsub\n",
    "2018-02-06 15:45:11 INFO uvcontsub\n",
    "2018-02-06 15:45:11 INFO uvcontsub\n",
    "2018-02-06 15:45:11 INFO uvcontsub\n",
    "2018-02-06 15:45:11 INFO uvcontsub\n",
    "2018-02-06 15:45:11 INFO SubMS::parseColumnNames() Using DATA column.\n",
    "```\n",
    "\n",
    "A few options are possible at this stage. User can let the process continue in serial, which depending on the size of the MS, can take long, and at the end the continuum subtracted output will be a normal MS. Depending on what the user wants to do next, there is the possibility to recreate the MMS using task [partition](https://casa.nrao.edu/casadocs-devel/stable/global-task-list/task_partition). If user only wants to run tclean and create an image, having either MS or MMS will work in the same way because [tclean](https://casa.nrao.edu/casadocs-devel/stable/global-task-list/task_tclean) can run in parallel regardless whether the input is MS or MMS.\n",
    "\n",
    " \n",
    "\n",
    "If the users opts to recreate the MMS before running uvcontsub, best recommend axis to do combine=\\'spw\\' is per scan. Partition will have to be called in the following way:\n",
    "\n",
    " \n",
    "\n",
    "```\n",
    "partition(vis='myMS.ms', outputvis='myout.ms', createmms=True, separationaxis='scan')\n",
    "```\n",
    "\n",
    "#####   {#section \n",
    "\n",
    "##### flagdata (with mode=\\'rflag\\') {#flagdata-with-moderflag \n",
    "\n",
    "The Rflag action=\\'calculate\\' can be used to produce the frequency and time thresholds in a first pass which can then be applied in a second pass, using action=\\'apply\\' once or several times. When this is done with the Multi-MS structure the thresholds calculated in the first pass might differ from the thresholds that would be calculated using a single MS structure. This is due to the fact that in the Multi-MS structure the data are partitioned into Sub-MSs. The default is to produce a balanced partition with respect to the SPWs and scans, with the aim to get content from all SPWs and scans into each of the Sub-MSs. For this reason, the statistics calculated by RFlag may differ across Sub-MSs, as they would differ for different data selections. At the moment this issue has not been assessed thoroughly for real-world datasets. A related question that is not understood in detail at the moment, and that can affect both serial and parallel runs of RFlag, is how much the thresholds can differ between the single pass and dual pass modes of RFlag.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples parallelization \n",
    "\n",
    "Examples of running CASA interactively or via a script in parallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel processing using Multi-MS (MMS) in CASA is unverified - please use at own discretion. \n",
    "\n",
    "##### [Please consider [[parallel imaging](https://casa.nrao.edu/casadocs-devel/stable/parallel-processing/parallel-imaging)]{ using normal MS as alternative.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of running CASA in parallel\n",
    "\n",
    "The following is a list of typical examples on how to run CASA in parallel. Once CASA is started with *mpicasa* and the \"Multi-MS\" is created, there is basically no difference between running CASA in serial and in parallel. You can find an example of a parallelized analysis in the alma-m100-analysis-hpc-regression.py script located in a sub-directory of your CASA distribution. For example, if CASA is untarred in /home/user/casa-release-5.0.0-el6, the alma-m100 script can be found in /home/user/casa-release-5.0.0-el6/lib/python2.7/regressions/\n",
    "\n",
    "``` {.verbatim}\n",
    "  alma-m100-analysis-hpc-regression.py\n",
    "```\n",
    "\n",
    "Example 1. Run the above regression script in parallel, using 8 cores in parallel and 1 core as the MPI Client.\n",
    "\n",
    "```\n",
    "mpicasa -n 9 <path_to_casa>/casa --nogui --log2term -c alma-m100-analysis-hpc-regression.py\n",
    "```\n",
    "\n",
    "Example 2. Start CASA as described before for an interactive session, using 5 cores on the local machine.\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <path_to_casa>/casa <casa-options>\n",
    "```\n",
    "\n",
    "An *xterm* will be open showing in the tile bar *rank0*. Rank 0 is where the MPIClient runs. The other 4 cores have been opened and are idle waiting for any activity to be sent to them. \n",
    "\n",
    "Run **importasdm** to create a \"Multi-MS\" and save the online flags to a file. The output will be automatically named uid\\_\\_A002_X888a.ms, which is an MMS partitioned across spw and scan. The online flags are saved in the file uid\\_\\_A002_X888a_cmd.txt.\n",
    "\n",
    "```\n",
    "CASA <2>: importasdm('uid__A002_X888a', createmms=True, savecmds=True)\n",
    "```\n",
    "\n",
    "List the contents of the MMS using **listobs**. In order to see how the MMS is partitioned, use **listpartition**.\n",
    "\n",
    "```\n",
    "CASA <3>: listobs('uid__A002_X888a.ms', listfile='uid__A002_X888a.listobs')\n",
    "CASA <4>: listpartition('uid__A002_X888a.ms')\n",
    "```\n",
    "\n",
    "Apply the online flags produced by **importasdm**, using **flagdata** in list mode. flagdata is parallelized therefore each engine will work on a separated \"Sub-MS\" to apply the flags from the uid\\_\\_A002_X888a_cmd.txt file. You will see messages in the terminal (also saved in the casa-\\#\\#\\#.log file), containing the strings MPIServer-1, MPIServer-2, etc., for all the cores that process in parallel.\n",
    "\n",
    "```\n",
    "CASA <5>: flagdata('uid__A002_X888a.ms', mode='list', inpfile='uid__A002_X888a_cmd.txt')\n",
    "```\n",
    "\n",
    "Flag auto-correlations and the high Tsys antenna also using list mode for optimization.\n",
    "\n",
    "```\n",
    "CASA <6>: flagdata('uid__A002_X888a.ms', mode='list',\n",
    "                   inpfile=[\"autocorr=True\",\"antenna='DA62'\"])\n",
    "```\n",
    "\n",
    "Create all calibration tables in the same way as for a normal MS. Task **gaincal** is not parallelized, therefore it will work on the MMS as if it was a normal MS.\n",
    "\n",
    "```\n",
    "CASA <7>: gaincal('uid__A002_X888a.ms', caltable='cal-delay_uid__A002_X888a.K',\n",
    "                  field='*Phase*',spw='1,3,5,7', solint='inf',combine='scan',\n",
    "                  refant=therefant, gaintable='cal-antpos_uid__A002_X888a',\n",
    "                  gaintype='K'))\n",
    "```\n",
    "\n",
    "Apply all the calibrations to the MMS. **applycal** will work in parallel on each \"Sub-MS\" using the available cores.\n",
    "\n",
    "```\n",
    "CASA <8>: applycal(vis='uid__A002_X888a.ms', field='0', spw='9,11,13,15',\n",
    "                   gaintable=['uid__A002_X888a.tsys',\n",
    "                              'uid__A002_X888a.wvr.smooth',\n",
    "                              'uid__A002_X888a.antpos'],\n",
    "                   gainfield=['0', '', ''], interp='linear,linear',\n",
    "                   spwmap=[tsysmap,[],[]], calwt=True, flagbackup=False)\n",
    "```\n",
    "\n",
    "Split out science spectral windows. Task **split** is also parallelized, therefore it will recognize that the input is an MMS and will process it in parallel, creating also an output MMS. \n",
    "\n",
    "```\n",
    "CASA <9>: split(vis='uid__A002_X888a.ms', outputvis='uid__A002_X888a.ms.split',\n",
    "                 datacolumn='corrected', spw='9,11,13,15', keepflags=True)\n",
    "```\n",
    "\n",
    "Run **tclean** normally to create your images.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Interface Framework \n",
    "\n",
    "The mpi4casa parallelization framework and advanced CASA parallel processing\n",
    "\n",
    "The CASA parallelization framework, *mpi4casa* was developed as a layer on top of MPI using a client-server model. The Client is the master process, driving user interaction, and dispatching user commands to the servers. Servers are all the other processes, running in the background, waiting for commands sent from the client side.\n",
    "\n",
    "One use-case of *mpi4casa* is to run CASA in parallel on a Multi-MS, as explained in previous chapters. There are other ways to process the data in parallel using *mpi4casa* without the need to create a Multi-MS. For instance, advanced users can benefit from the *mpi4casa* implementation to run multiple task commands in different cores or nodes.\n",
    "\n",
    " \n",
    "\n",
    "##### **Initialization**\n",
    "\n",
    "Start CASA in parallel as explained in previous chapters, using *mpicasa*.\n",
    "\n",
    "Import MPICommandClient from *mpi4casa* *module*\n",
    "\n",
    "```\n",
    "from mpi4casa.MPICommandClient import MPICommandClient\n",
    "```\n",
    "\n",
    "Create an instance of MPICommandClient\n",
    "\n",
    "```\n",
    "client = MPICommandClient()\n",
    "```\n",
    "\n",
    "Set logging policy\n",
    "\n",
    "```\n",
    "client.set_log_mode('redirect')\n",
    "```\n",
    "\n",
    "Initialize command handling services\n",
    "\n",
    "```\n",
    "client.start_services()\n",
    "```\n",
    "\n",
    "\n",
    "##### Syntax to send a command request\n",
    "\n",
    "```\n",
    "ret = client.push_command_request(command,block,target_server,parameters)\n",
    "```\n",
    "\n",
    "*command:* String containing the Python/CASA command to be executed. The command parameters can be included within the command in itself also as strings.\n",
    "\n",
    "*block*: Boolean to control whether command request is executed in blocking mode (True) or in non-blocking mode (False). Default is False (non-blocking).\n",
    "\n",
    "*target_server*: List of integers corresponding to the server IDs to handle the command\n",
    "\n",
    "target_server=None: The command will be executed by the first available server\n",
    "\n",
    "target_server=2: The command will be executed by the server n \\#2 as soon as it is available\n",
    "\n",
    "target_server=\\[0,1\\]: The command will be executed by the servers n \\#2 and \\#3\n",
    "\n",
    "*parameters (Optional):* Alternatively the command parameters can be specified in a separated dictionary using their native types instead of strings.\n",
    "\n",
    "ret (Return Variable):\n",
    "\n",
    "In non-blocking mode: It will not block and will return an Integer (command ID) to retrieve the command response at a later stage.\n",
    "\n",
    "In blocking mode: It will block until the list of dictionaries, containing the command response is received.\n",
    "\n",
    " \n",
    "\n",
    "##### Syntax to receive a command result\n",
    "\n",
    "```\n",
    "ret = client.get_command_response(command_request_id_list,block)\n",
    "```\n",
    "\n",
    "*command_request_id_list*: List of Ids (integers) corresponding to the commands whose result is to be retrieved.\n",
    "\n",
    "*block*: Boolean to control whether command request is executed in blocking mode (True) or in non-blocking mode (False).\n",
    "\n",
    "ret (Return Variable): List of dictionaries, containing the response parameters. The dictionary elements are as follows:\n",
    "\n",
    "'successful' (Boolean): indicates whether command execution was successful or failed\n",
    "\n",
    "'traceback' (String): In case of failure contains the traceback of the exception thrown\n",
    "\n",
    "'ret': Contains the result of the command in case of successful execution\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Run **wvrgcal** in 2 different MeasurementSets (for instance each one corresponding to an Execution Block):\n",
    "\n",
    "```\n",
    "#Example of full command including parameters\n",
    "cmd1 = \"wvrgcal(vis='X54.ms',caltable='cal-wvr_X54',spw=[1,3,5,7])\"\n",
    "cmdId1 = client.push_command_request(cmd1,block=False)\n",
    "\n",
    "#Example of command with separated parameter list\n",
    "cmd2 = \"wvrgcal()\"\n",
    "params2={'vis':'X54.ms','caltable':'cal-wvr_X54','spw':[1,3,5,7]}\n",
    "cmdId2 = client.push_command_request(cmd2,block=False,parameters=params2)\n",
    "\n",
    "#Retrieve results\n",
    "resultList = client.get_command_response([cmdId1, cmdId2],block=True)\n",
    "```\n",
    "\n",
    "**Note:** *target_server* is not specified because these are monolithic state-less commands, therefore any server can process them.\n",
    "\n",
    "Example 2:Use the CASA ms tool to get the data from 2 EBs and apply a custom median filter:\n",
    "\n",
    "```\n",
    "#Open MSs\n",
    "client.push_command_request(\"tb.open('x54.ms')\",target_server=1)\n",
    "client.push_command_request(\"tb.open('x220.ms')\",target_server=2)\n",
    "\n",
    "#Apply median filter\n",
    "client.push_command_request(\"data=ms.getcell('DATA',1)\",target_server=[1,2])\n",
    "client.push_command_request(\"from scipy import signal\",target_server=[1,2])\n",
    "client.push_command_request(\"filt_data=signal.medfilt(data)\",target_server=[1,2])\n",
    "\n",
    "#Put filter data back in the MSs\n",
    "client.push_command_request(\"tb.putcell('DATA',1,filt_data)\",target_server=[1,2])\n",
    "\n",
    "#Close MSs\n",
    "client.push_command_request(\"tb.close(),target_server=[1,2],block=True)\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "NOTE: *target_server* is specified as each command depends on the state generated by previous ones; *block* will block only on the last commands as all the others will be executed using a FIFO queue, meaning the commands will be received in the same order they were sent.\n",
    "</div>\n",
    "\n",
    " \n",
    "\n",
    "Link to first version of the CASA framework development [document](https://svn.cv.nrao.edu/view/casa/trunk/gcwrap/python/scripts/mpi4casa/CASA-4.3-MPI-Parallel-Processing-Framework-Installation-and-advance-user-guide.pdf \"CASA Framework development\")\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with Multi-MS \n",
    "\n",
    "How to create and run functional tests using Multi-MSs\n",
    "\n",
    "\\--CASA Developer\\--\n",
    "\n",
    "This document is a quick guide on how to run CASA functional tests using Multi-MSs. The Multi-MS (MMS) data sets are not stored in the data repository as to avoid duplicating space. Each developer can create a reference set of Multi-MSs for their tests and use them when updating the code.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**NOTE**: The user needs to start CASA with *mpicasa* in order to run it in parallel. Note that in most of the functional tests, the execution will not be faster when running on MMS, mostly because the datasets of the tests are small and the overhead of setting up the cluster may dominate.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Create Multi-MS for the tests\n",
    "\n",
    "There are 3 ways to create Multi-MS data sets for the functional tests (those run using the runUnitTest.py tool).\n",
    "\n",
    "1.  Create MMS automatically for the MSs used in a particular test script.\n",
    "2.  Create MMS for a given directory containing MSs.\n",
    "3.  Use the partition task to have full control and create the Multi-MSs manually.\n",
    "\n",
    "##### Create Multi-MSs automatically using a script\n",
    "\n",
    "Use the make_mmsdata.py script, which uses the **convertToMMS** class (explained in next section) to create MMS. This script will create MMS for all MSs used in a specific test using default parameters of the partition task. The developer can modify some parameters of partition when running the script. The make_mmsdata.py script is installed in \\<trunk/gcwrap/scripts/python/regressions/admin\\>. The make_mmsdata.py scripts contains the following options:\n",
    "\n",
    "  ---------------------------------------------------------------------- ----------------------------------------------------------------------------------------\n",
    "  *no option*     Print this message and exit.\n",
    "  *\\--all*        Create MMS for all tasks in TASKLIST.\n",
    "  *\\--ignore*     From all tasks, do no create MMS for the given \\<tasks\\>\n",
    "  *\\--list*       Print the list of tasks from TASKLIST and exit.\n",
    "  *\\--axis*       partition *separationaxis* to use (*spw*, *scan*, *auto*); *default=auto*\n",
    "  *\\--numsubms*   Number of subMSs to use when creating MMS; *default=4*\n",
    "  *\\<tasks\\>*     Create MMS data for the given tasks. Additional tasks are separated by white spaces.  \n",
    "  ---------------------------------------------------------------------- ----------------------------------------------------------------------------------------\n",
    "\n",
    " \n",
    "\n",
    "Examples\n",
    "\n",
    "**\\<script-path\\>** can be for CASA trunk: \\<trunk/gcwrap/scripts/python/regressions/admin/\\> or for a tarball: \\<casa-prerelease\\.../lib/python2.7/\\>\n",
    "\n",
    " \n",
    "\n",
    "\\> Get the usage information\n",
    "\n",
    "```\n",
    "casa --c <script-path>make_mmsdata.py\n",
    "```\n",
    "\n",
    "\\> List the tasks for which this script can create MMS.\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <install-path>casa --c <script-path>make_mmsdata.py --list\n",
    "```\n",
    "\n",
    "\\> Create MMS data for gaincal tests using the default auto *separationaxis* and 4 Sub-MSs.\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <install-path>casa --c <script-path>make_mmsdata.py gaincal\n",
    "```\n",
    "\n",
    "\\> Create Multi-MS data for all tasks defined in the script, except flagdata and split, and create 8 Sub-MSs.\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <install-path>casa --c <script-path>make_mmsdata.py --numsubms=8 --ignore flagdata split\n",
    "```\n",
    "\n",
    " \\> Create MMS data for all tasks defined in TASKLIST using the scan *separationaxis*\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <install-path>casa --c <script-path>make_mmsdata.py --axis=scan --all\n",
    "```\n",
    "\n",
    "##### Create Multi-MSs semi-automatically using the convertToMMS() Python class\n",
    "\n",
    "Use the **convertToMMS** class inside CASA directly to create Multi-MSs. This class will create MMS data for all the MeasurementSets of a given directory. It will ignore any non-MS data such as calibration tables. Start CASA with *mpicasa* and do the following:.\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <install-path>casa --nogui --log2term\n",
    "```\n",
    "\n",
    "```\n",
    "CASA>: import partitionhelper as ph\n",
    "CASA>: ph.convertToMMS()\n",
    "```\n",
    "\n",
    "   Options:\n",
    "\n",
    "   *inpdir* \\<*dir*\\>\n",
    "\n",
    "directory with input MS.\n",
    "\n",
    "   *mmsdir* \\<*dir*\\>\n",
    "\n",
    "directory to save output MMS. If not given, it will save the MMS in a directory called mmsdir in the current directory.\n",
    "\n",
    "   *axis=\\'auto\\'*\n",
    "\n",
    "*separationaxis* parameter of partition (*spw*,*scan*,*auto*).\n",
    "\n",
    "   *numsubms='auto'*\n",
    "\n",
    "number of subMSs to create in output MMS\n",
    "\n",
    "   *cleanup=False*\n",
    "\n",
    "if True it will remove the output directory before starting.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    " NOTE: this script will run using the default values of partition. It will try to create an MMS for every MS in the input directory. It will skip non-MS directories such as cal tables. If partition succeeds, the script will create a link to every non-MS file in the output directory. The script will not walk through sub-directories of *inpdir*. It will also skip files or directories that start with a .\n",
    "</div>\n",
    "\n",
    "Examples:\n",
    "\n",
    "\\> Create Multi-MSs for all MSs present in the given directory and save them to the default directory \"mmsdir\".\n",
    "\n",
    "```\n",
    "CASA>: ph.convertoToMMS(inpdir='$CASADATA/regressions/unittest/bandpass')\n",
    "```\n",
    "\n",
    "##### Create Multi-MSs manually using partition\n",
    "\n",
    "Run task **partition** manually to create Multi-MSs by hand inside CASA and have more control on the parameters of the task. See help **partition** for more details.\n",
    "\n",
    "Example:\n",
    "\n",
    "\\> Start CASA with *mpicasa* to create an MMS for the Four_ants_3C286.ms test MS and select only the DATA column. Create flag backup and choose the spw *separationaxis*. Use **listpartition** to see the content of the MMS.\n",
    "\n",
    "```\n",
    "CASA >: partition('Four_and_3C286.ms', outputvis='mytest.ms', separationaxis='spw', datacolumn='DATA', flagbackup=True)\n",
    "```\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Modify the functional tests to run with Multi-MSs\n",
    "\n",
    "In order to run the existing functional tests with a different data set, there is an option in runUnitTest.py, which will look for input data in a different location other than that defined in the tests themselves. The script runUnitTest.py will set an environmental variable called TEST_DATADIR when it is called with the option *\\--datadir*. This variable can be read by the tests to use a different location for the input data sets.\n",
    "\n",
    "Add the following lines in the beginning of the test script. See examples in test_flagdata.py.\n",
    "\n",
    "    # Path for data\n",
    "    datapath = os.environ.get('CASAPATH').split()[0] + \"/data/regression/unittest/flagdata/\"\n",
    "    # Pick up alternative data directory to run tests on MMSs\n",
    "    testmms = False\n",
    "    if os.environ.has_key('TEST_DATADIR'):  \n",
    "       DATADIR = str(os.environ.get('TEST_DATADIR'))+'/flagdata/'\n",
    "       if os.path.isdir(DATADIR):\n",
    "           testmms = True\n",
    "           datapath = DATADIR\n",
    "           print 'flagdata tests will use data from '+datapath      \n",
    "\n",
    "This assumes that the MMS data is stored under a sub-directory with the task name. Most of the existing functional tests follow the recommended way of storing MSs in the data repository, under \\<CASADATA\\>/regression/unittest/\\<taskname\\>. Tests that read data from other locations need to be adjusted accordingly. One easier option is to create symbolic links to MSes from other locations to the standard place in \\<CASADATA\\>/regression/unittest/\\<taskname\\>.\n",
    "\n",
    "The following tests already support MMS such as described above:\n",
    "\n",
    "``` {.verbatim}\n",
    "test_bandpass          test_clearstat \n",
    "test_concat            test_conjugatevis \n",
    "test_cvel2             test_flagdata \n",
    "test_fluxscale         test_gaincal \n",
    "test_gencal            test_hanningsmooth\n",
    "test_listhistory       test_listobs \n",
    "test_listvis           test_plotms \n",
    "test_split             test_uvcontsub\n",
    "test_virtualconcat     test_vishead\n",
    "test_visstat \n",
    "```\n",
    "\n",
    "For these tests, one only needs to create MMS and run the tests with the *\\--datadir* option.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Run the tests with Multi-MSs\n",
    "\n",
    "Run the tests as you normally do to check that they all pass. Create the MMSs as described in Section 1 and run the same tests on the new data sets. If the MMS are created under ./unittest_mms/\\<taskname\\>, run the script as follows:\n",
    "\n",
    "```\n",
    "mpicasa -n 5 <install-path>casa --c <script-path>runUnitTest.py  --datadir=./unittest_mms test_taskname\n",
    "```\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Troubleshooting\n",
    "\n",
    "Q. The tests pass using normal MSs but fail on Multi-MSs.\n",
    "\n",
    "A. The first thing to check is if the *separationaxis* used to **partition** the MS is appropriate to the processing done by the task."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
